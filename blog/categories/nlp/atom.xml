<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: nlp | Shogo's Blog]]></title>
  <link href="https://shogo82148.github.io/blog/categories/nlp/atom.xml" rel="self"/>
  <link href="https://shogo82148.github.io/"/>
  <updated>2017-12-06T05:45:00+09:00</updated>
  <id>https://shogo82148.github.io/</id>
  <author>
    <name><![CDATA[Shogo Ichinose]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[TinySegmenterをLaTeXに移植してみた]]></title>
    <link href="https://shogo82148.github.io/blog/2012/12/16/tinysegmenter-for-tex/"/>
    <updated>2012-12-16T13:11:00+09:00</updated>
    <id>https://shogo82148.github.io/blog/2012/12/16/tinysegmenter-for-tex</id>
    <content type="html"><![CDATA[<p>この記事は<a href="http://atnd.org/events/34318">TeX &amp; LaTeX Advent Calendar</a>の傘下記事です．
15日はk16.shikanoさんの「<a href="http://note.golden-lucky.net/2012/12/tex.html">TeX がむかついたので実装したけど挫折してる話</a>」,
17日は@egtraさんの「<a href="http://dev.activebasic.com/egtra/2012/12/18/522/">LCDF TypetoolsでOpenTypeフォントを使う(DVIPDFMXで)</a>」です．</p>

<p>neruko3114が参加しているのを見てなんだか楽しそうだったで参加してみました．
とはいってもネタも思いつかなったので，過去に作ったものをTeXに移植してみました．
ターゲットは<a href="http://chasen.org/~taku/software/TinySegmenter/">TinySegmenter</a>．
以前作った<a href="https://github.com/shogo82148/TinySegmenterMaker">TinySegmenterMaker</a>でLaTeXを出力できるようになったよ！</p>

<!-- More -->


<h2>使ってみる</h2>

<p><a href="https://github.com/shogo82148/TinySegmenterMaker">TinySegmenterMaker</a>のレポジトリをダウンロードするなり<code>git clone</code>するなりして
落としてきます．
レポジトリに入っているのはモデルファイルとスクリプトだけです．
これらを使ってTeXのスタイルファイルを作ります．</p>

<pre><code class="bash">$ cd /path/to/TinySegmenterMaker/
$ ./maker tex &lt; RWCP.model
</code></pre>

<p>カレントディレクトリにtinysegmenter.styができます．
TeX から見えるところにおいておきましょう．
これを使うソースコードは次のようになります．</p>

<pre><code class="tex">\documentclass{jarticle}
\usepackage{tinysegmenter}
\begin{document}
\TinySegmenter{-}{私の名前は中野です}
\end{document}
</code></pre>

<p>platexで処理するとこんな感じに表示されるはず．</p>

<pre><code class="plain">私-の-名前-は-中野-です
</code></pre>

<h2>仕組み</h2>

<p>TinySegmeneterは元の文章の一部を切り取ってハッシュに入れる動作をしている．
でも，LaTeXにはハッシュみたいなデータ構造がないのでコントロールシーケンスで代用．
<code>\@ifundefined</code>で有無を確認し，<code>\csname\endcsname</code>で置き換え．
コントロールシーケンスの一部に日本語を使わないといけないので，日本語LaTeX環境でしか動かない．
ただ，一部句点などの扱いが違う？よくわからない．</p>

<p>あとは，文字種の取得が必要なんだけど，ここでも同じことをしてます．
すべてのアルファベット・ひらがな・カタカナ・数字について，その文字種をベタ書き．
それ以外は全部漢字扱い．
そのため，それ以外の文字を使うとオリジナルとは違う結果になるかも．</p>

<p>最後は足し算．これはカウンタを使えば簡単ですね．</p>

<h2>応用編</h2>

<p>TinySegmenterMakerでは自由にモデルを差し替えることができます．
以前JavaScript版のTinySegmenterを使って，
<a href="http://shogo82148.github.com/blog/2012/12/05/kikoemasuka/">聞こえますか…自動生成…してみた…よ…</a>
ということをしてみました．
LaTeXだってできるはず．</p>

<p><a href="https://github.com/shogo82148/kikoemasuka">聞こえますか…</a>
に心に呼びかけるためのモデルファイルが含まれています．
これをダウンロードして読み込ませます．</p>

<pre><code class="bash">$ ./maker tex &lt; model
</code></pre>

<p>これを自分のドキュメントに読み込ませてみます．</p>

<pre><code class="tex">\documentclass{jarticle}
\usepackage{tinysegmenter}
\begin{document}
(…\TinySegmenter{…}{聞こえますか聞こえますかあなたの心に直接語りかけています}…)
\end{document}
</code></pre>

<p>私の声が聞こえましたか・・・？</p>

<pre><code class="plain">(…聞こえますか…聞こえますか…あなたの…心に…直接語りかけています…)
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MeCabをPythonから使う注意点とか]]></title>
    <link href="https://shogo82148.github.io/blog/2012/12/15/mecab-python/"/>
    <updated>2012-12-15T17:38:00+09:00</updated>
    <id>https://shogo82148.github.io/blog/2012/12/15/mecab-python</id>
    <content type="html"><![CDATA[<p>日本語の文章をコンピュータで色々いじるときに，
必ずと言っていいほどよく使うのが形態素解析器．
スペースなどの明示的な区切りの無い日本語を単語に分割してくれるツールです．
中でもMeCabが非常に有名で，さまざまなところで使われています．</p>

<p>MeCabはいろいろな言語から呼び出すことができます．
自然言語処理の分野ではPythonが人気のようですね．僕も使っています．
しかし，MeCabをPythonから使う場合，注意する点がいくつかあります．
そこにハマっている後輩を見かけたので，文章として残しておくことにします．
Python2系が対象です(3系はよくわからない)．
注意するのは以下の二点です．</p>

<ul>
<li>MeCabに渡す文字列はencode，戻ってきた文字列はdecodeする</li>
<li>MeCabに渡した文字列は必ず変数に入れておく</li>
</ul>


<!-- More -->


<h2>EncodeとDecode</h2>

<p>Python2系の文字列には，バイト列として扱われる文字列(str)と，Unicodeで表現された文字列(unicode)があります．
日本語を扱う場合，strだといろいろ問題があるので，特に理由がなければunicodeを使うべきです．
しかし，MeCabはstrしか受け付けません．
そこでMeCabに渡す直前・直後でencode・decodeするようにします．</p>

<pre><code class="python">import MeCab
tagger = MeCab.tagger('-Owakati')
text = u'MeCabで遊んでみよう！'

result = tagger.parse(text) # エラー！

encoded_text = text.encode('utf-8') # encodeが必要
encoded_result = tagger.parse(text)
result = result.decode('utf-8') # 必ずdecode
</code></pre>

<p><code>'utf-8'</code>の部分は辞書の文字コードに合わせて適宜書き換えてください．
デフォルトはeuc-jpですが，utf-8の方が幸せになれると思います．</p>

<h2>必ず変数に入れる</h2>

<p>次にMeCabの作ったノードに直接アクセスして，品詞情報などを取ってくることを考えます．
適当に書いてみるとこんな感じでしょうか．</p>

<pre><code class="python">import MeCab
tagger = MeCab.tagger('')
text = u'MeCabで遊んでみよう！'

node = tagger.parseToNode(text.encode('utf-8'))
while node:
    #printはstrを渡す必要があるのでdecodeは不要
    print node.surface + '\t' + node.feature
    node = node.next
</code></pre>

<p>MeCabに渡す直前にencodeもしているので上手く動きそうです．
(decodeしてないのはprintに渡すためなので気にしなくておｋ)
しかし，このコードの出力は下のような悲惨なものとなるのです
(ブラウザさんに配慮して一部修正，環境によっても違うと思います)</p>

<pre><code class="plain">        BOS/EOS,*,*,*,*,*,*,*,*
MeCab   名詞,一般,*,*,*,*,*
        ??   助詞,格助詞,一般,*,*,*,で,デ,デ
?詞,?   動詞,自立,*,*,五段・バ行,連用タ接続,遊ぶ,アソン,アソン
???     助詞,接続助詞,*,*,*,*,で,デ,デ
??,*,*       動詞,非自立,*,*,一段,未然ウ接続,みる,ミヨ,ミヨ
,*,     助動詞,*,*,*,不変化型,基本形,う,ウ,ウ
*,*     記号,一般,*,*,*,*,！,！,！
        BOS/EOS,*,*,*,*,*,*,*,*
</code></pre>

<p>なぜこのようなことが起きてしまったのでしょう？
答えは<code>text.encode('utf-8')</code>の戻り値の寿命と，MeCabノードの構造にあります．</p>

<p>みんなさんが普段お使いのPythonは，C言語で実装されたCPythonだと思います．
「CPythonでは、ガベージコレクションの方式として参照カウント方式とマーク・アンド・スイープ方式を併用」しています
(<a href="http://ja.wikipedia.org/wiki/Python#.E3.83.87.E3.83.BC.E3.82.BF.E5.9E.8B">Python - Wikipedia</a>)．
参照カウント方式おかげでCPythonは不要になったオブジェクトを不要になった瞬間に検出し，そのオブジェクトを解放することができます．
つまり実際には5行目を少し細かく見ると，Pythonは以下の処理をします．</p>

<ol>
<li><code>text.encode('utf-8')</code>を呼び出し，"エンコード済みtext"を作成</li>
<li><code>tagger.parseToNode</code>を呼び出し，結果を<code>node</code>に代入</li>
<li>不要になった<strong> &ldquo;エンコード済みtext"を破棄 </strong></li>
</ol>


<p>ポイントは3番ですね．6行目を実行する前に，"エンコード済みtext"は破棄されてしまいます．</p>

<p>さて，次にMeCabがどのようにノードの情報を扱っているか見てみましょう．
MeCabの言語バインディングのページには，ノードのsurfaceは文字列型であるような定義が書いてありますが，あれは嘘です．
<a href="http://code.google.com/p/mecab/source/browse/trunk/mecab/src/mecab.h">ソース</a>を見ればわかりますが，みんな大好きポインタとして定義されています．
実はこのポインタ，<strong> &ldquo;エンコード済みtext"上の開始点を指し示しています </strong>．</p>

<p>つまり，どういうことかというと，</p>

<ul>
<li>MeCabはsurfaceを作るのに毎回"エンコード済みtext"からコピペしてた</li>
<li>しかし，MeCabはPythonにそのことを伝えていなかった</li>
<li>不要と判断したPythonによって"エンコード済みtext"はすでに破棄されており，そこには何もなかった</li>
</ul>


<p>これを解決するにはPythonに"エンコード済みtext"が使用中であることを伝え，破棄されないようにする必要があります．
一番簡単な方法は変数に保存しておくことです．変数のスコープにいる間は"エンコード済みtext"が破棄される心配はありません．</p>

<pre><code class="python">import MeCab
tagger = MeCab.tagger('')
text = u'MeCabで遊んでみよう！'

encoded_text = text.encode('utf-8')
node = tagger.parseToNode(encoded_text) # 変数に入れる！
while node:
    print node.surface + '\t' + node.feature
    node = node.next
</code></pre>

<p>これで上手く行きます．</p>

<p>encode_textとnodeの寿命が一致している必要があります．
nodeの結果を何度も利用する場合は一度nodeの内容をすべてPythonのリストか何かに格納しましょう．
一度変換してしまえば，ガーベージコレクションは正しく動きます．</p>

<h2>まとめ</h2>

<ul>
<li>MeCabに渡す文字列はencode，戻ってきた文字列はdecodeする</li>
<li>MeCabに渡した文字列は必ず変数に入れておく</li>
</ul>


<p>面倒なのでラッパーを書くかといいかもしれませんね．
もしくは自前で実装とか．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TinySegmenterの学習ツールを作ってみた]]></title>
    <link href="https://shogo82148.github.io/blog/2012/11/23/tinysegmentermaker/"/>
    <updated>2012-11-23T14:37:00+09:00</updated>
    <id>https://shogo82148.github.io/blog/2012/11/23/tinysegmentermaker</id>
    <content type="html"><![CDATA[<p><a href="http://chasen.org/~taku/software/TinySegmenter/">TinySegmenter</a>は工藤さん作のJavaScriptだけで書かれたコンパクトな分かち書きソフトウェアです．
わずか20kバイト程度のサイズしかなく，お手軽に使える分かち書きソフトウェアですが，
当たり前のことながら学習データに使った新聞記事以外の文章の精度はイマイチ．
改善しようにも学習用のプログラムが公開されていないのでモデルの修正が大変です．</p>

<p>ないなら作ってしまいましょう！</p>

<!-- More -->


<h2>ダウンロード</h2>

<p>ソースはgithubで公開しています．cloneするなりzipファイルを落としてくるなりしてください．</p>

<ul>
<li><a href="https://github.com/shogo82148/TinySegmenterMaker">TinySegmenterMaker</a></li>
</ul>


<h2>学習方法</h2>

<p>スペースで分かち書きしたコーパスをあらかじめ準備しておきます．
コーパスから分かち書きの情報と素性を取り出します．</p>

<pre><code class="bash">$ ./extract &lt; corpus.txt &gt; features.txt
</code></pre>

<p>AdaBoostを用いて学習します．
新しい弱分類器の分類精度が0.001以下，繰り返し回数が10000回以上となったら学習を終了します．</p>

<pre><code class="bash">$ g++ -O3 -o train train.cpp # コンパイル
$ ./train -t 0.001 -n 10000 features.txt model # 学習
</code></pre>

<p>きちんと分割できるが実際に試してみます．</p>

<pre><code class="bash">$ ./segment model
私の名前は中野です
私 の 名前 は 中野 です
</code></pre>

<h2>ライブラリの作成</h2>

<p><a href="http://chasen.org/~taku/software/TinySegmenter/">TinySegmenter</a>は実装が簡単なためいろいろな言語へ移植されています．
モデルの更新のたびにそれらへの言語の移植バージョンを作るのは大変です．
というわけで，makerコマンドで各種言語用のライブラリを作れます．
学習結果のモデルはライブラリのなかに組み込まれ，ファイル単体で簡単に使用することができます．
allを指定することで，対応しているすべての言語向けのライブラリを出力します．</p>

<pre><code class="bash">$ ./maker javascript &lt; model
$ ./maker perl &lt; model
$ ./maker ruby &lt; medel
$ ./maker python &lt; model
$ ./maker cpp &lt; model
$ ./maker all &lt; model # 上のライブラリをすべて作成します
</code></pre>

<h2>AdaBoostについておさらい</h2>

<p><a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>が面白そうだと前々から思っていて，
現実逃避にこれを使っているという<a href="http://chasen.org/~taku/software/TinySegmenter/">TinySegmenter</a> の実装をしてみました．
簡単に<a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>についてお勉強しておきましょう．</p>

<p>性能が悪い分類器だってたくさん集まれば高性能な分類器になれるはず！という
三人よれば文殊の知恵みたいな考え方としてBoostingがあります．
その学習方法の一つが<a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>です．</p>

<h3>初期化</h3>

<p>データxが与えられたときのyを推定する問題を考えます．
あらかじめm個の学習データが与えられたとしましょう．</p>

<p>{% math %}
(x_1, y_1), \cdots, (x_m, y_m) ただし，y_i \in {-1, +1}
{% endmath %}</p>

<p>与えられたすべてのデータを判別するのが最終目標ですが，
弱い分類器にそこまで求めるのは酷です．
そこで分類器の評価の際に，間違えては困るデータに重み付けをすることにします．
その重みをDとしましょう．
はじめはどれが重要かわからないので重みは全て同じ，
扱いやすいように合計1になるように正規化しておきます．</p>

<p>{% math %}
D_1(i) = \frac{1}{m}, i = 1, \cdots, m.
{% endmath %}</p>

<h3>弱分類器の選択</h3>

<p>弱分類器 h_t を選びます．
弱分類器の作り方はなんでもいいのですが，+1か-1を結果として返し，{% m %}| 0.5 - \epsilon_t|{% em %} がなるべく大きくなるようなものを選びます．
{% m %}\epsilon_t{% em %} は分類に失敗したデータの割合(重み考慮)で，次式で定義されます．</p>

<p>{% math %}
\epsilon_t = \sum_{i=1}^m D_t(i) [y_i \not = h_t]
{% endmath %}</p>

<p>εが0か1に近い分類器は分類性能が高いということなので，最終的な分類器を構成するときに重みを大きくしたほうがいいような気がします．
εが1に近いということはほとんどの分類に失敗しているということなので，性能が高いというのは変な気がするけど，
必ず間違うのなら負号を反対にしてしまえばいいのです．</p>

<p>整理すると，重み関数の条件は，0か1に近くなると絶対値が大きく，0.5以上で負，0.5以下で正となるような関数，ということになります．
このような条件を満たす関数は無限に考えられますが，次の式使うといいらしいです(論理的背景があるんだろうけど理解はしていない)．</p>

<p>{% math %}
\alpha_t = \frac{1}{2} \log \frac{1-\epsilon_t}{\epsilon}
{% endmath %}</p>

<h3>重みの更新</h3>

<p>弱分類器の選択を何度も繰り返せば性能がどんどん上がっていくはずですが，ただ単に良い分類器を選ぶだけでは性能は上がりません．
今ある弱分類器で正しく分類できなかったデータを正しく分類してくれる(つまり今の欠点を補ってくれる)
弱分類器を次の候補とするべきです．</p>

<p>そこで，うまく分類できたデータの重みを少なく，分類できなかったデータの重みを大きくしましょう．</p>

<p>{% math %}
D_{t+1}(i) = \frac{D_t(i)\exp (-\alpha_t y_i h_t(x_i))}{Z_t}
{% endmath %}</p>

<p>Z_tは正規化のための係数です．
弱分類器の選択と重みの更新を繰り返すことで分類精度が上がっていきます．</p>

<h3>分類する</h3>

<p>最終的な分類器は
{% math %}
H(x) = \sum_{t=1}^T \alpha_t h_t(x)
{% endmath %}
となります．H(x)の負号が分類器の予測結果です．</p>

<h2>実演</h2>

<p>数式だけだとわかりにくと思うので分類の実演．
青丸と赤丸をAdaBoostで分類するテスト．弱分類器として座標軸と平行な直線を選択します．
重みの大きいデータは色が濃くなります．
分類に失敗しているデータは色が濃く，優先的に分類されることを確認できると思います．</p>

<script type="text/javascript" src="http://jsdo.it/blogparts/1XZV/js"></script>


<h2>単語分割に応用</h2>

<p>さて，学習の方法が分かったので単語分割への応用を考えます．
<a href="http://chasen.org/~taku/software/TinySegmenter/">TinySegmenter</a>は，単語分割を文字と文字の間を分割するか/しないかという二値分類の問題と捉えます．
分類の手がかりとなる素性には，前後の文字列，文字種，以前の分類結果を使っています．</p>

<p>弱分類器には素性の有無を使っています．
このようなxが○以上とか素性がある/ないみたいな
ANDとかORを使わずに作れる弱分類器を 決定株(decision stump) というらしいです．
こうすることで弱分類器の重みが，そのまま素性の重みになります．</p>

<h2>再学習機能</h2>

<p><a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>の更新式を眺めていると</p>

<p>{% math %}
D_{T}(i) = \frac{\exp (H(x_i))}{Z_T}
{% endmath %}</p>

<p>と表すことができるということがわかります．
これはつまり，学習済みのモデルから学習中に使う変数がすべて復元できるということです．</p>

<p>この考えに基づいてTinySegmenterMakerでは再学習機能を実装してみました．</p>

<pre><code class="bash">$ ./train -t 0.0001 -n 10000 -M model_old features.txt model_new
</code></pre>

<p>この機能を使えば10000回とりあえず回してみて，不十分だからもう10000回追加，みたいなことができます．
同じコーパスを使っていれば，10000+10000回と20000回の結果はほぼ同じ結果になるはずです(もちろん浮動小数点演算の誤差が多少あるけど)．</p>

<p>この変数の復元は，元のモデルの学習に使ったものとは違うコーパスでも可能なので，
リポジトリに登録されているモデルを元に自分で用意したコーパスで学習することも可能なはず．
(どの程度性能が変わるのかとか論理的な解析・評価はしてないので，実際使えるものなのかはよくわからない)</p>

<p>オリジナルTinySegmenterのモデルからも再学習は可能だけど，こちらはスケールをいじってあるから再学習の効果はもっとよくわからない．</p>

<h2>ダイナミックプログラミング版TinySegmenter</h2>

<p>分類器が返すスコアの絶対値は分類の確信度を表していると仮定して，
文章全体の確信度の合計が最大になる単語分割を採用するプログラムも書いてみた(tinysegmenter.dp.jp)．
AdaBoost自身がこの値をデータの重み付けに使っているくらいなので，この仮定はだいたいあっているんじゃないか
と勝手に考えているけど，論理的な裏付けをとったわけじゃないし性能評価もしてない．</p>

<h2>参考</h2>

<ul>
<li><a href="http://chasen.org/~taku/software/TinySegmenter/">TinySegmenter</a></li>
<li><a href="http://en.wikipedia.org/wiki/AdaBoost">Adaboost</a></li>
<li><a href="http://ultraist.hatenablog.com/entry/20120603/1338675881">BimyouSegmenter</a></li>
<li><a href="http://search.cpan.org/dist/Text-TinySegmenter/">Text::TinySegmenter</a></li>
<li><a href="http://www.programming-magic.com/20080726203844/">TinySegmenterをPythonで書いてみた</a></li>
<li><a href="http://d.hatena.ne.jp/llamerada/20080224/1203818061">TinySegmenterをRubyに移植</a></li>
<li><a href="http://d.hatena.ne.jp/repeatedly/20101105/1288946662">TinySegmenter in D</a></li>
<li><a href="http://code.google.com/p/tinysegmenter-cpp/">tinysegmenter-cpp</a></li>
<li><a href="http://www.programming-magic.com/20080816010106/">PHP版TinySegmenter作ってみた</a></li>
<li><a href="http://pub.ne.jp/arihagne/?entry_id=2768818">VBAでTinySegmenterしてみる</a></li>
<li><a href="http://wiki.dobon.net/index.php?free%2FTinySegmenter.NET">TinySegmenter.NET : 分かち書きを行うC#のクラス</a></li>
<li><a href="http://blog.bornneet.com/Entry/276/">TinySegmenterをiPhone(Objective-C)に移植してみました</a></li>
<li><a href="http://blog.bornneet.com/Entry/277/">TinySegmenter.mをRegexKitLiteに対応させてみた</a></li>
<li><a href="https://code.google.com/p/cmecab-java/">cmecab-java MeCabのJavaバインディング＋Lucene/Solr用トークナイザ・フィルタ</a>

<ul>
<li>おんなじインターフェースでTinySegmenterも使えるみたい</li>
<li><a href="https://code.google.com/p/cmecab-java/source/browse/trunk/src/net/moraleboost/tinysegmenter/TinySegmenter.java">TinySegmenter.java</a></li>
</ul>
</li>
<li><a href="http://miyamuko.s56.xrea.com/xyzzy/tiny-segmenter.html">tiny-segmenter - xyzzy Lisp だけで実装されたコンパクトな分かち書きソフトウェア</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PythonでCaboChaを美味しくいただく]]></title>
    <link href="https://shogo82148.github.io/blog/2012/11/01/cabocha/"/>
    <updated>2012-11-01T23:02:00+09:00</updated>
    <id>https://shogo82148.github.io/blog/2012/11/01/cabocha</id>
    <content type="html"><![CDATA[<p>日本語構文解析器<a href="http://code.google.com/p/cabocha/">CaboCha</a>をPythonから使ってみたメモ．</p>

<!-- More -->


<h2>インストール</h2>

<p><a href="http://code.google.com/p/cabocha/">CaboCha</a>自体のインストールは公式のドキュメントを参照．
ググれば他の人のレポートも出てくるはず．</p>

<p><a href="http://code.google.com/p/cabocha/">CaboCha</a>のソースコードを展開したディレクトリの中の
pythonディレクトリにPython-bindingが入ってます．
そこに移動した後，管理者権限で以下のコマンドを実行すればインストール完了．</p>

<pre><code class="bash">python setup.py install
</code></pre>

<h2>食べ方</h2>

<h3>解析結果を文字列出力</h3>

<p>python/test.py に書いてあるとおり．</p>

<pre><code class="python test.py http://code.google.com/p/cabocha/source/browse/trunk/python/test.py">#!/usr/bin/python
# -*- coding: utf-8 -*-

import CaboCha

# c = CaboCha.Parser("");
c = CaboCha.Parser()

sentence = "太郎はこの本を二郎を見た女性に渡した。"

print c.parseToString(sentence)

tree =  c.parse(sentence)

print tree.toString(CaboCha.FORMAT_TREE)
print tree.toString(CaboCha.FORMAT_LATTICE)
</code></pre>

<p>以下のような結果が得られれば成功．</p>

<pre><code class="plain">&lt;PERSON&gt;太郎&lt;/PERSON&gt;は-----------D
                     この-D       |
                       本を---D   |
                       二郎を-D   |
                           見た-D |
                           女性に-D
                           渡した。
EOS

&lt;PERSON&gt;太郎&lt;/PERSON&gt;は-----------D
                     この-D       |
                       本を---D   |
                       二郎を-D   |
                           見た-D |
                           女性に-D
                           渡した。
EOS

* 0 6D 0/1 2.909358
太郎  名詞,固有名詞,人名,名,*,*,太郎,タロウ,タロー   B-PERSON
は 助詞,係助詞,*,*,*,*,は,ハ,ワ    O
* 1 2D 0/0 1.257926
この  連体詞,*,*,*,*,*,この,コノ,コノ    O
* 2 4D 0/1 0.638994
本 名詞,一般,*,*,*,*,本,ホン,ホン O
を 助詞,格助詞,一般,*,*,*,を,ヲ,ヲ   O
* 3 4D 1/2 1.696047
二 名詞,数,*,*,*,*,二,ニ,ニ  O
郎 名詞,一般,*,*,*,*,郎,ロウ,ロー O
を 助詞,格助詞,一般,*,*,*,を,ヲ,ヲ   O
* 4 5D 0/1 0.000000
見 動詞,自立,*,*,一段,連用形,見る,ミ,ミ   O
た 助動詞,*,*,*,特殊・タ,基本形,た,タ,タ  O
* 5 6D 0/1 0.000000
女性  名詞,一般,*,*,*,*,女性,ジョセイ,ジョセイ  O
に 助詞,格助詞,一般,*,*,*,に,ニ,ニ   O
* 6 -1D 0/1 0.000000
渡し  動詞,自立,*,*,五段・サ行,連用形,渡す,ワタシ,ワタシ  O
た 助動詞,*,*,*,特殊・タ,基本形,た,タ,タ  O
。 記号,句点,*,*,*,*,。,。,。   O
EOS
</code></pre>

<p><code>tree.toString(CaboCha.FORMAT_XML)</code>でXML形式の出力も可能です．</p>

<pre><code class="xml">&lt;sentence&gt;
 &lt;chunk id="0" link="6" rel="D" score="2.909358" head="0" func="1"&gt;
  &lt;tok id="0" feature="名詞,固有名詞,人名,名,*,*,太郎,タロウ,タロー" ne="B-PERSON"&gt;太郎&lt;/tok&gt;
  &lt;tok id="1" feature="助詞,係助詞,*,*,*,*,は,ハ,ワ" ne="O"&gt;は&lt;/tok&gt;
 &lt;/chunk&gt;
 &lt;chunk id="1" link="2" rel="D" score="1.257926" head="2" func="2"&gt;
  &lt;tok id="2" feature="連体詞,*,*,*,*,*,この,コノ,コノ" ne="O"&gt;この&lt;/tok&gt;
 &lt;/chunk&gt;
 &lt;chunk id="2" link="4" rel="D" score="0.638994" head="3" func="4"&gt;
  &lt;tok id="3" feature="名詞,一般,*,*,*,*,本,ホン,ホン" ne="O"&gt;本&lt;/tok&gt;
  &lt;tok id="4" feature="助詞,格助詞,一般,*,*,*,を,ヲ,ヲ" ne="O"&gt;を&lt;/tok&gt;
 &lt;/chunk&gt;
 &lt;chunk id="3" link="4" rel="D" score="1.696047" head="6" func="7"&gt;
  &lt;tok id="5" feature="名詞,数,*,*,*,*,二,ニ,ニ" ne="O"&gt;二&lt;/tok&gt;
  &lt;tok id="6" feature="名詞,一般,*,*,*,*,郎,ロウ,ロー" ne="O"&gt;郎&lt;/tok&gt;
  &lt;tok id="7" feature="助詞,格助詞,一般,*,*,*,を,ヲ,ヲ" ne="O"&gt;を&lt;/tok&gt;
 &lt;/chunk&gt;
 &lt;chunk id="4" link="5" rel="D" score="0.000000" head="8" func="9"&gt;
  &lt;tok id="8" feature="動詞,自立,*,*,一段,連用形,見る,ミ,ミ" ne="O"&gt;見&lt;/tok&gt;
  &lt;tok id="9" feature="助動詞,*,*,*,特殊・タ,基本形,た,タ,タ" ne="O"&gt;た&lt;/tok&gt;
 &lt;/chunk&gt;
 &lt;chunk id="5" link="6" rel="D" score="0.000000" head="10" func="11"&gt;
  &lt;tok id="10" feature="名詞,一般,*,*,*,*,女性,ジョセイ,ジョセイ" ne="O"&gt;女性&lt;/tok&gt;
  &lt;tok id="11" feature="助詞,格助詞,一般,*,*,*,に,ニ,ニ" ne="O"&gt;に&lt;/tok&gt;
 &lt;/chunk&gt;
 &lt;chunk id="6" link="-1" rel="D" score="0.000000" head="12" func="13"&gt;
  &lt;tok id="12" feature="動詞,自立,*,*,五段・サ行,連用形,渡す,ワタシ,ワタシ" ne="O"&gt;渡し&lt;/tok&gt;
  &lt;tok id="13" feature="助動詞,*,*,*,特殊・タ,基本形,た,タ,タ" ne="O"&gt;た&lt;/tok&gt;
  &lt;tok id="14" feature="記号,句点,*,*,*,*,。,。,。" ne="O"&gt;。&lt;/tok&gt;
 &lt;/chunk&gt;
&lt;/sentence&gt;
</code></pre>

<p>しかし，このXML形式，<code>&amp;</code>や<code>"</code>，<code>&lt;</code>, <code>&gt;</code>などの特殊記号を置換してくれないので，
この結果をXMLのパーサに通す場合などは注意が必要．</p>

<p>そもそも標準の辞書ではこれらの文字を上手く扱えないので前処理を行ったほうがいいのかもしれない．
半角の<code>&amp;</code>は辞書に登録されていおらず，全角の＆にする必要がある．</p>

<h3>Treeの中身をいじってみる</h3>

<p>一度文字列に変換してしまうと色々面倒なことが起こりそうなので，Treeの中身を直接いじってみる．
ドキュメントが無いので<a href="http://code.google.com/p/cabocha/source/browse/trunk/src/cabocha.h">cabocha.h</a>
の中身を見ながら試してみました．</p>

<pre><code class="python">#!/usr/bin/python
# -*- coding: utf-8 -*-

import CaboCha
c = CaboCha.Parser()

sentence = "太郎はこの本を渡した。"

tree =  c.parse(sentence)

for i in range(tree.chunk_size()):
    chunk = tree.chunk(i)
    print 'Chunk:', i
    print ' Score:', chunk.score
    print ' Link:', chunk.link
    print ' Size:', chunk.token_size
    print ' Pos:', chunk.token_pos
    print ' Head:', chunk.head_pos # 主辞
    print ' Func:', chunk.func_pos # 機能語
    print ' Features:',
    for j in range(chunk.feature_list_size):
        print chunk.feature_list(j),
    print
    print

for i in range(tree.token_size()):
    token = tree.token(i)
    print 'Surface:', token.surface
    print ' Normalized:', token.normalized_surface
    print ' Feature:', token.feature
    print ' NE:', token.ne # 固有表現
    print ' Info:', token.additional_info
    print ' Chunk:', token.chunk
    print
</code></pre>
]]></content>
  </entry>
  
</feed>
